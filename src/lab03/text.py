import re

def normalize(text: str, casefold: bool, yo2e: bool):
    text = ' '.join(text.split())
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace("—ë","–µ").replace('–Å', '–ï')
    return text

def tokenize(text: str):
    text=text.replace(',', ' ').replace('.',' ')
    return re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9-\s]', '', text).split()

def count_freq(tokens: list[str]):
    freq = {}
    for i in tokens:
        if i in freq:
            freq[i] += 1
        else:
            freq[i] = 1
    return freq

def top_n(freq: dict[str, int], n: int):
    s = sorted(freq.items(), key=lambda x: (-x[1], x[0]))
    return s[:n]

if __name__ == "__main__":
    print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", True,False))
    print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", False,True))
    print(normalize("Hello\r\nWorld", False,False))
    print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", False,False))
    print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
    print(tokenize("hello,world!!!"))
    print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
    print(tokenize("2025 –≥–æ–¥"))
    print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
    print(count_freq(["a","b","a","c","b","a"]))
    print(top_n(count_freq(["a","b","a","c","b","a"]),2))
    print((count_freq(["bb","aa","bb","aa","cc"])))
    print(top_n(count_freq(["bb","aa","bb","aa","cc"]),2))